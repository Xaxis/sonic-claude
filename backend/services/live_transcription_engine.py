"""
Live Transcription Engine
Real-time audio-to-Sonic-Pi code generation with source separation
"""
import asyncio
import time
import numpy as np
import librosa
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import tempfile
import soundfile as sf

# Make aubio optional since it can fail to build on some systems
try:
    import aubio
    AUBIO_AVAILABLE = True
except ImportError:
    AUBIO_AVAILABLE = False
    logger = None  # Will be set later

from backend.core import get_logger
from backend.models.transcription import (
    StemType, Note, Beat, StemAnalysis, SonicPiCode,
    TranscriptionStatus, LiveTranscriptionResult, TranscriptionSettings
)

logger = get_logger(__name__)


class LiveTranscriptionEngine:
    """
    Real-time audio transcription engine that:
    1. Captures audio from input device
    2. Separates into stems (drums, bass, vocals, other)
    3. Analyzes each stem (pitch, rhythm, timbre)
    4. Generates musical data for playback
    5. Streams results via WebSocket
    """

    def __init__(self, audio_engine=None):
        """Initialize transcription engine"""
        self.audio_engine = audio_engine  # Will be replaced with new audio engine
        self.is_running = False
        self.current_status = TranscriptionStatus.IDLE
        self.settings = TranscriptionSettings()
        self.audio_buffer = []
        self.sample_rate = 48000
        
        logger.info("âœ… LiveTranscriptionEngine initialized")
    
    async def transcribe_audio_buffer(
        self,
        audio_data: np.ndarray,
        sample_rate: int,
        stems_enabled: Dict[StemType, bool],
        settings: TranscriptionSettings
    ) -> LiveTranscriptionResult:
        """
        Main transcription pipeline:
        1. Source separation
        2. Stem analysis
        3. Code generation
        """
        start_time = time.time()
        
        try:
            self.current_status = TranscriptionStatus.ANALYZING
            
            # Step 1: Separate into stems
            logger.info("ðŸŽµ Separating audio into stems...")
            self.current_status = TranscriptionStatus.SEPARATING
            stems = await self._separate_stems(audio_data, sample_rate, stems_enabled)
            
            # Step 2: Analyze each stem
            logger.info("ðŸ” Analyzing stems...")
            self.current_status = TranscriptionStatus.TRANSCRIBING
            stem_analyses = []

            for stem_type, stem_audio in stems.items():
                if stems_enabled.get(stem_type, False):
                    # Analyze stem
                    analysis = await self._analyze_stem(
                        stem_type, stem_audio, sample_rate, settings
                    )
                    stem_analyses.append(analysis)

            processing_time = time.time() - start_time
            self.current_status = TranscriptionStatus.COMPLETE

            logger.info(f"âœ… Transcription complete in {processing_time:.2f}s")

            # Return result with empty code fields (will be generated by audio engine)
            return LiveTranscriptionResult(
                status=TranscriptionStatus.COMPLETE,
                stems=stem_analyses,
                sonic_pi_code=[],  # No longer generating Sonic Pi code
                combined_code="",  # Will be generated by audio engine
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"âŒ Transcription failed: {e}")
            self.current_status = TranscriptionStatus.ERROR
            return LiveTranscriptionResult(
                status=TranscriptionStatus.ERROR,
                stems=[],
                sonic_pi_code=[],
                combined_code="",
                processing_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def _separate_stems(
        self,
        audio_data: np.ndarray,
        sample_rate: int,
        stems_enabled: Dict[StemType, bool]
    ) -> Dict[StemType, np.ndarray]:
        """
        Separate audio into stems using Demucs
        For now, we'll use a simplified approach with frequency-based separation
        Full Demucs integration can be added later for production
        """
        # Convert to mono if stereo
        if len(audio_data.shape) > 1:
            audio_mono = audio_data.mean(axis=1)
        else:
            audio_mono = audio_data
        
        # Simplified stem separation using frequency bands
        # This is a placeholder - real implementation would use Demucs
        stems = {}
        
        # Drums: High-pass filter for transients
        if stems_enabled.get(StemType.DRUMS, False):
            stems[StemType.DRUMS] = await self._extract_drums(audio_mono, sample_rate)
        
        # Bass: Low-pass filter
        if stems_enabled.get(StemType.BASS, False):
            stems[StemType.BASS] = await self._extract_bass(audio_mono, sample_rate)
        
        # Vocals: Mid-range focus
        if stems_enabled.get(StemType.VOCALS, False):
            stems[StemType.VOCALS] = await self._extract_vocals(audio_mono, sample_rate)
        
        # Other: Residual
        if stems_enabled.get(StemType.OTHER, False):
            stems[StemType.OTHER] = audio_mono
        
        return stems

    async def _extract_drums(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """Extract drum-like transients using onset detection"""
        # Use high-pass filter to isolate percussive elements
        audio_filtered = librosa.effects.preemphasis(audio)
        return audio_filtered

    async def _extract_bass(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """Extract bass frequencies (20-250 Hz)"""
        # Low-pass filter
        bass = librosa.effects.harmonic(audio)
        return bass

    async def _extract_vocals(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """Extract vocal-range frequencies"""
        # Band-pass filter for vocal range (80-1000 Hz)
        return audio  # Simplified for now

    async def _analyze_stem(
        self,
        stem_type: StemType,
        audio: np.ndarray,
        sample_rate: int,
        settings: TranscriptionSettings
    ) -> StemAnalysis:
        """Analyze a single stem to extract musical features"""

        # Detect tempo and beats
        tempo, beats = await self._detect_tempo_and_beats(audio, sample_rate)

        # Detect notes (pitch + timing)
        notes = await self._detect_notes(audio, sample_rate, settings)

        # Estimate key
        key = await self._estimate_key(audio, sample_rate)

        # Calculate energy
        energy = float(np.sqrt(np.mean(audio ** 2)))

        # Get dominant frequencies
        dominant_freqs = await self._get_dominant_frequencies(audio, sample_rate)

        return StemAnalysis(
            stem_type=stem_type,
            notes=notes,
            beats=beats,
            tempo=tempo,
            key=key,
            time_signature="4/4",  # Default for now
            dominant_frequencies=dominant_freqs,
            energy=energy
        )

    async def _detect_tempo_and_beats(
        self, audio: np.ndarray, sr: int
    ) -> Tuple[float, List[Beat]]:
        """Detect tempo and beat positions using librosa"""
        # Estimate tempo
        tempo, beat_frames = librosa.beat.beat_track(y=audio, sr=sr)

        # Convert frames to time
        beat_times = librosa.frames_to_time(beat_frames, sr=sr)

        # Create Beat objects
        beats = []
        for i, time in enumerate(beat_times):
            beats.append(Beat(
                time=float(time),
                strength=1.0,  # Simplified
                is_downbeat=(i % 4 == 0)  # Assume 4/4 time
            ))

        return float(tempo), beats

    async def _detect_notes(
        self, audio: np.ndarray, sr: int, settings: TranscriptionSettings
    ) -> List[Note]:
        """Detect notes using pitch detection and onset detection"""
        notes = []

        # Detect onsets
        onset_frames = librosa.onset.onset_detect(
            y=audio,
            sr=sr,
            units='frames',
            backtrack=True
        )
        onset_times = librosa.frames_to_time(onset_frames, sr=sr)

        # Detect pitches using pyin
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio,
            fmin=librosa.note_to_hz('C2'),
            fmax=librosa.note_to_hz('C7'),
            sr=sr
        )

        # Match onsets with pitches
        for i, onset_time in enumerate(onset_times):
            # Find pitch at onset
            onset_frame = int(onset_time * sr / 512)  # hop_length=512

            if onset_frame < len(f0) and voiced_flag[onset_frame]:
                pitch_hz = f0[onset_frame]
                if not np.isnan(pitch_hz):
                    midi_note = librosa.hz_to_midi(pitch_hz)
                    note_name = librosa.midi_to_note(int(midi_note))

                    # Estimate duration (until next onset or end)
                    if i < len(onset_times) - 1:
                        duration = onset_times[i + 1] - onset_time
                    else:
                        duration = 0.5  # Default

                    # Filter by minimum duration
                    if duration >= settings.min_note_duration:
                        notes.append(Note(
                            pitch=float(midi_note),
                            note_name=note_name,
                            onset_time=float(onset_time),
                            duration=float(duration),
                            velocity=0.8,  # Simplified
                            confidence=float(voiced_probs[onset_frame])
                        ))

        return notes

    async def _estimate_key(self, audio: np.ndarray, sr: int) -> str:
        """Estimate musical key using chroma features"""
        chroma = librosa.feature.chroma_cqt(y=audio, sr=sr)
        chroma_mean = chroma.mean(axis=1)

        # Simple key estimation based on strongest chroma
        notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        key_idx = int(np.argmax(chroma_mean))
        return notes[key_idx]

    async def _get_dominant_frequencies(
        self, audio: np.ndarray, sr: int, n_freqs: int = 5
    ) -> List[float]:
        """Get top N dominant frequencies"""
        # Compute FFT
        fft = np.fft.rfft(audio)
        magnitude = np.abs(fft)
        freqs = np.fft.rfftfreq(len(audio), 1/sr)

        # Get top N peaks
        peak_indices = np.argsort(magnitude)[-n_freqs:][::-1]
        dominant_freqs = [float(freqs[i]) for i in peak_indices]

        return dominant_freqs

    async def _generate_sonic_pi_code(
        self, analysis: StemAnalysis, settings: TranscriptionSettings
    ) -> SonicPiCode:
        """
        Generate Sonic Pi code for a stem analysis
        This integrates with the existing Sonic Pi OSC system
        """
        stem_type = analysis.stem_type

        # Choose synth based on stem type
        synth_map = {
            StemType.DRUMS: "drum_heavy_kick",
            StemType.BASS: "bass_foundation",
            StemType.VOCALS: "prophet",
            StemType.OTHER: "blade"
        }
        synth_name = synth_map.get(stem_type, "beep")

        # Generate live_loop name
        loop_name = f"transcribed_{stem_type.value}"

        # Build code based on stem type
        if stem_type == StemType.DRUMS:
            code = self._generate_drum_code(analysis, loop_name, settings)
        elif stem_type == StemType.BASS:
            code = self._generate_bass_code(analysis, loop_name, synth_name, settings)
        elif stem_type == StemType.VOCALS:
            code = self._generate_melodic_code(analysis, loop_name, synth_name, settings)
        else:
            code = self._generate_harmonic_code(analysis, loop_name, synth_name, settings)

        # Extract parameters
        parameters = {
            "tempo": analysis.tempo,
            "key": analysis.key,
            "energy": analysis.energy,
            "note_count": len(analysis.notes)
        }

        return SonicPiCode(
            stem_type=stem_type,
            code=code,
            live_loop_name=loop_name,
            synth_name=synth_name,
            parameters=parameters
        )

    def _generate_drum_code(
        self, analysis: StemAnalysis, loop_name: str, settings: TranscriptionSettings
    ) -> str:
        """Generate drum pattern code"""
        tempo = analysis.tempo
        beats = analysis.beats

        # Create rhythm pattern from beats
        pattern = []
        if beats:
            # Group beats into bars (assuming 4/4)
            bar_duration = 60.0 / tempo * 4  # 4 beats per bar

            for beat in beats[:16]:  # Limit to 16 beats
                if beat.is_downbeat:
                    pattern.append("sample :bd_haus, amp: 1.5")
                else:
                    pattern.append("sample :drum_snare_soft, amp: 1.0")

        code = f"""live_loop :{loop_name} do
  use_bpm {tempo:.0f}

  # Transcribed drum pattern
  {chr(10).join(f"  {p}" for p in pattern[:4])}
  sleep 1
end"""

        return code

    def _generate_bass_code(
        self, analysis: StemAnalysis, loop_name: str, synth_name: str, settings: TranscriptionSettings
    ) -> str:
        """Generate bass line code"""
        tempo = analysis.tempo
        notes = analysis.notes

        # Filter for bass range notes (below C3)
        bass_notes = [n for n in notes if n.pitch < 60]

        if not bass_notes:
            return f"# No bass notes detected"

        # Quantize if enabled
        if settings.quantize_enabled:
            bass_notes = self._quantize_notes(bass_notes, settings.quantize_resolution, tempo)

        # Generate note sequence
        note_lines = []
        for note in bass_notes[:8]:  # Limit to 8 notes
            note_lines.append(
                f"play :{note.note_name.lower()}, release: {note.duration:.2f}, amp: {note.velocity:.2f}"
            )
            note_lines.append(f"sleep {note.duration:.2f}")

        code = f"""live_loop :{loop_name} do
  use_bpm {tempo:.0f}
  use_synth :{synth_name}

  # Transcribed bass line
  {chr(10).join(f"  {line}" for line in note_lines)}
end"""

        return code

    def _generate_melodic_code(
        self, analysis: StemAnalysis, loop_name: str, synth_name: str, settings: TranscriptionSettings
    ) -> str:
        """Generate melodic code for vocals/leads"""
        tempo = analysis.tempo
        notes = analysis.notes

        if not notes:
            return f"# No notes detected"

        # Quantize if enabled
        if settings.quantize_enabled:
            notes = self._quantize_notes(notes, settings.quantize_resolution, tempo)

        # Generate note sequence
        note_lines = []
        for note in notes[:16]:  # Limit to 16 notes
            note_lines.append(
                f"play :{note.note_name.lower()}, release: {note.duration:.2f}, amp: {note.velocity:.2f}"
            )
            note_lines.append(f"sleep {note.duration:.2f}")

        code = f"""live_loop :{loop_name} do
  use_bpm {tempo:.0f}
  use_synth :{synth_name}

  # Transcribed melody
  {chr(10).join(f"  {line}" for line in note_lines)}
end"""

        return code

    def _generate_harmonic_code(
        self, analysis: StemAnalysis, loop_name: str, synth_name: str, settings: TranscriptionSettings
    ) -> str:
        """Generate harmonic/chord code"""
        tempo = analysis.tempo
        key = analysis.key

        # Simple chord progression in detected key
        code = f"""live_loop :{loop_name} do
  use_bpm {tempo:.0f}
  use_synth :{synth_name}

  # Harmonic progression in {key}
  play_chord [:{key.lower()}3, :{key.lower()}4, :{key.lower()}5], release: 2
  sleep 2
end"""

        return code

    def _quantize_notes(
        self, notes: List[Note], resolution: float, tempo: float
    ) -> List[Note]:
        """Quantize note timings to grid"""
        beat_duration = 60.0 / tempo
        grid_duration = beat_duration * resolution

        quantized = []
        for note in notes:
            # Quantize onset time
            quantized_onset = round(note.onset_time / grid_duration) * grid_duration

            # Quantize duration
            quantized_duration = max(
                grid_duration,
                round(note.duration / grid_duration) * grid_duration
            )

            quantized.append(Note(
                pitch=note.pitch,
                note_name=note.note_name,
                onset_time=quantized_onset,
                duration=quantized_duration,
                velocity=note.velocity,
                confidence=note.confidence
            ))

        return quantized

    def _combine_sonic_pi_code(self, codes: List[SonicPiCode]) -> str:
        """Combine all stem codes into a single Sonic Pi program"""
        if not codes:
            return "# No code generated"

        header = """# Live Transcribed Performance
# Generated by Sonic Claude Live Transcription Engine
# This code recreates the analyzed audio in real-time

"""

        all_loops = "\n\n".join(code.code for code in codes)

        return header + all_loops

    # Removed send_to_sonic_pi - will be replaced with audio engine playback

